{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Модель для обработки CSV"
      ],
      "metadata": {
        "id": "GoCQBMv8iaP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Чтение и обработка данных валидационного датасета\n",
        "def load_validation_data(validation_file_path):\n",
        "    with open(validation_file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    parsed_data = []\n",
        "\n",
        "    for line in lines:\n",
        "        cleaned_line = line.strip().replace('\"', '').strip()\n",
        "        split_line = cleaned_line.split(',')\n",
        "        parsed_data.append(split_line)\n",
        "\n",
        "    validation_df = pd.DataFrame(parsed_data, columns=['filename', 'position', 'is_header'])\n",
        "    validation_df['is_header'] = pd.to_numeric(validation_df['is_header'], errors='coerce')\n",
        "    validation_df['is_header'] = validation_df['is_header'].fillna(0).astype(int)\n",
        "\n",
        "    return validation_df\n",
        "\n",
        "# Функция для извлечения признаков для одной строки\n",
        "def extract_single_row_features(row, row_index):\n",
        "    num_cells = row.count()  # Число ячеек\n",
        "    avg_cell_length = row.apply(lambda x: len(str(x))).mean()  # Средняя длина ячейки\n",
        "    num_chars = row.apply(lambda x: len(str(x).replace(' ', ''))).sum()  # Число символов\n",
        "    pct_numeric = row.apply(lambda x: sum(c.isdigit() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_alpha = row.apply(lambda x: sum(c.isalpha() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_symbol = row.apply(lambda x: sum(not c.isalnum() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_numeric_cells = sum(row.apply(lambda x: str(x).replace('.', '').isdigit())) / num_cells\n",
        "    pct_empty_cells = sum(row.isna()) / num_cells  # Процент пустых ячеек\n",
        "    pct_uppercase_cells = sum(row.apply(lambda x: str(x).isupper())) / num_cells  # Процент ячеек с верхним регистром\n",
        "    unique_char_count = row.apply(lambda x: len(set(str(x)))).sum()  # Количество уникальных символов\n",
        "    return pd.Series([num_cells, avg_cell_length, num_chars, pct_numeric, pct_alpha, pct_symbol, pct_numeric_cells, pct_empty_cells, pct_uppercase_cells, unique_char_count, row_index])\n",
        "\n",
        "# Функция для обработки одной таблицы с учетом меток заголовков\n",
        "def process_table(file, validation_df):\n",
        "    # Чтение данных из CSV, использование on_bad_lines для игнорирования ошибок\n",
        "    df = pd.read_csv(file, sep=';', on_bad_lines='skip', engine='python')\n",
        "\n",
        "    # Добавляем столбец 'is_header' и заполняем значениями 0\n",
        "    df['is_header'] = 0\n",
        "\n",
        "    # Получаем название файла для фильтрации меток заголовков\n",
        "    filename = os.path.basename(file).replace('.csv', '')\n",
        "\n",
        "    # Фильтруем строки из валидационного набора данных для текущего файла\n",
        "    file_validation_data = validation_df[validation_df['filename'].str.contains(filename)]\n",
        "\n",
        "    # Проставляем метки заголовков на основе информации из валидационного датасета\n",
        "    for _, row in file_validation_data.iterrows():\n",
        "        position = row['position'].strip()\n",
        "        header_index = int(row['is_header'])\n",
        "\n",
        "        if position == 'top':\n",
        "            if header_index < len(df):\n",
        "                df.loc[header_index, 'is_header'] = 1\n",
        "        elif position == 'left':\n",
        "            df.iloc[:, header_index] = 1\n",
        "\n",
        "    # Извлечение признаков для каждой строки\n",
        "    features_df = df.apply(lambda row: extract_single_row_features(row, df.index.get_loc(row.name)), axis=1)\n",
        "\n",
        "    # Добавление признаков к оригинальной таблице\n",
        "    feature_columns = ['num_cells', 'avg_cell_length', 'num_chars', 'pct_numeric', 'pct_alpha', 'pct_symbol', 'pct_numeric_cells', 'pct_empty_cells', 'pct_uppercase_cells', 'unique_char_count', 'row_index']\n",
        "    features_df.columns = feature_columns\n",
        "\n",
        "    # Объединяем признаки и метки заголовков\n",
        "    df = pd.concat([features_df, df[['is_header']]], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Основная функция для обработки всех файлов\n",
        "def process_all_files(data_folder, validation_file_path, output_path):\n",
        "    # Загрузка данных валидационного набора\n",
        "    validation_df = load_validation_data(validation_file_path)\n",
        "\n",
        "    # Обработка всех CSV файлов\n",
        "    files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
        "    all_data_with_features = []\n",
        "\n",
        "    for file in files:\n",
        "        processed_df = process_table(file, validation_df)  # Обработка каждой таблицы\n",
        "        processed_df['filename'] = os.path.basename(file)  # Добавляем имя файла для идентификации\n",
        "        all_data_with_features.append(processed_df)  # Добавляем таблицу в список\n",
        "\n",
        "    # Конкатенация всех таблиц в один общий DataFrame\n",
        "    fedot_dataset = pd.concat(all_data_with_features, ignore_index=True)\n",
        "\n",
        "    # Сохранение итогового DataFrame с признаками в новый CSV файл\n",
        "    fedot_dataset.to_csv(output_path, index=False, sep=';')\n",
        "    print(f\"Обработка завершена, файл сохранён как '{output_path}'\")\n",
        "\n",
        "# Пример использования:\n",
        "data_folder = '/content/data'\n",
        "validation_file_path = '/content/CTA_SAUS189_gt.csv'\n",
        "output_path = 'fedot_training_dataset.csv'\n",
        "\n",
        "process_all_files(data_folder, validation_file_path, output_path)\n",
        "\n",
        "# Гиперпараметрический тюнинг для модели RandomForest\n",
        "def model_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Пример вызова функции тюнинга (вставить после разделения данных)\n",
        "# tuned_model = model_tuning(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxcYOnmq-Fz-",
        "outputId": "5f4feb69-bea7-493b-af00-5a846351ae09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработка завершена, файл сохранён как 'fedot_training_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попытка сбалансировать классы и использовать градиентный бустинг"
      ],
      "metadata": {
        "id": "0CxO9j0hLAvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textdistance\n",
        "pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFc4bjyPQj46",
        "outputId": "ef22ef5b-3813-467b-8575-716420bb1cd2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-2.1.1-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Функция для извлечения признаков для одной строки\n",
        "def extract_single_row_features(row):\n",
        "    num_cells = row.count()  # Число ячеек\n",
        "    avg_cell_length = row.apply(lambda x: len(str(x))).mean()  # Средняя длина ячейки\n",
        "    num_chars = row.apply(lambda x: len(str(x).replace(' ', ''))).sum()  # Число символов\n",
        "    pct_numeric = row.apply(lambda x: sum(c.isdigit() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_alpha = row.apply(lambda x: sum(c.isalpha() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_symbol = row.apply(lambda x: sum(not c.isalnum() for c in str(x))).sum() / num_chars if num_chars > 0 else 0\n",
        "    pct_numeric_cells = sum(row.apply(lambda x: str(x).replace('.', '').isdigit())) / num_cells\n",
        "    pct_empty_cells = sum(row.isna()) / num_cells  # Процент пустых ячеек\n",
        "    pct_uppercase_cells = sum(row.apply(lambda x: str(x).isupper())) / num_cells  # Процент ячеек с верхним регистром\n",
        "    unique_char_count = row.apply(lambda x: len(set(str(x)))).sum()  # Количество уникальных символов\n",
        "\n",
        "    return pd.Series([num_cells, avg_cell_length, num_chars, pct_numeric, pct_alpha, pct_symbol, pct_numeric_cells, pct_empty_cells, pct_uppercase_cells, unique_char_count])\n",
        "\n",
        "# Функция для обработки одной таблицы (реальные данные)\n",
        "def process_real_table(file):\n",
        "    # Чтение данных из CSV\n",
        "    df = pd.read_csv(file, sep=';', on_bad_lines='skip', engine='python')\n",
        "\n",
        "    # Извлечение признаков для каждой строки\n",
        "    features_df = df.apply(extract_single_row_features, axis=1)\n",
        "\n",
        "    # Добавление признаков к оригинальной таблице\n",
        "    feature_columns = ['num_cells', 'avg_cell_length', 'num_chars', 'pct_numeric', 'pct_alpha', 'pct_symbol', 'pct_numeric_cells', 'pct_empty_cells', 'pct_uppercase_cells', 'unique_char_count']\n",
        "    features_df.columns = feature_columns\n",
        "\n",
        "    # Возвращаем DataFrame с признаками\n",
        "    return features_df\n",
        "\n",
        "# Основная функция для обработки всех файлов реальных данных\n",
        "def process_real_data(data_folder, output_path):\n",
        "    # Обработка всех CSV файлов\n",
        "    files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
        "    all_data_with_features = []\n",
        "\n",
        "    for file in files:\n",
        "        processed_df = process_real_table(file)  # Обработка каждой таблицы\n",
        "        processed_df['filename'] = os.path.basename(file)  # Добавляем имя файла для идентификации\n",
        "        all_data_with_features.append(processed_df)  # Добавляем таблицу в список\n",
        "\n",
        "    # Конкатенация всех таблиц в один общий DataFrame\n",
        "    real_data_with_features = pd.concat(all_data_with_features, ignore_index=True)\n",
        "\n",
        "    # Сохранение итогового DataFrame с признаками в новый CSV файл\n",
        "    real_data_with_features.to_csv(output_path, index=False, sep=';')\n",
        "    print(f\"Обработка завершена, файл сохранён как '{output_path}'\")\n",
        "\n",
        "# Пример использования:\n",
        "data_folder = '/content/realdata'\n",
        "output_path = 'real_data_with_features.csv'\n",
        "\n",
        "process_real_data(data_folder, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fuubJuah7AN",
        "outputId": "b157b913-c58c-49d0-91a9-52e6470307e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработка завершена, файл сохранён как 'real_data_with_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np  # Добавляем numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Загрузка данных\n",
        "df = pd.read_csv('fedot_training_dataset.csv', sep=';')\n",
        "\n",
        "# Разделение признаков и целевой переменной\n",
        "X = df.drop(columns=['is_header', 'filename'])\n",
        "y = df['is_header']\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Преобразуем список в numpy массив\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Модель Random Forest с балансировкой классов\n",
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Отчет о классификации и точность для Random Forest\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
        "print(f\"Accuracy RF: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
        "\n",
        "# Модель SVM с балансировкой классов\n",
        "svm_model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Отчет о классификации и точность для SVM\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, zero_division=0))\n",
        "print(f\"Accuracy SVM: {accuracy_score(y_test, y_pred_svm):.2f}\")\n",
        "\n",
        "# Модель XGBoost с балансировкой классов\n",
        "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=class_weight_dict[0]/class_weight_dict[1])\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Отчет о классификации и точность для XGBoost\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, zero_division=0))\n",
        "print(f\"Accuracy XGBoost: {accuracy_score(y_test, y_pred_xgb):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlCFgi5QK_rC",
        "outputId": "c83d170f-fc03-441f-ed38-8a988deaa334"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98      1675\n",
            "           1       0.84      0.88      0.86       187\n",
            "\n",
            "    accuracy                           0.97      1862\n",
            "   macro avg       0.91      0.93      0.92      1862\n",
            "weighted avg       0.97      0.97      0.97      1862\n",
            "\n",
            "Accuracy RF: 0.97\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      1675\n",
            "           1       0.64      0.98      0.78       187\n",
            "\n",
            "    accuracy                           0.94      1862\n",
            "   macro avg       0.82      0.96      0.87      1862\n",
            "weighted avg       0.96      0.94      0.95      1862\n",
            "\n",
            "Accuracy SVM: 0.94\n",
            "XGBoost Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98      1675\n",
            "           1       0.91      0.74      0.82       187\n",
            "\n",
            "    accuracy                           0.97      1862\n",
            "   macro avg       0.94      0.87      0.90      1862\n",
            "weighted avg       0.97      0.97      0.97      1862\n",
            "\n",
            "Accuracy XGBoost: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Загрузка реальных данных для предсказания\n",
        "real_data_path = '/content/real_data_with_features.csv'\n",
        "real_data = pd.read_csv(real_data_path, sep=';')\n",
        "\n",
        "# Предположим, что реальные данные имеют те же признаки, что и тренировочные\n",
        "X_real = real_data.drop(columns=['is_header', 'filename'], errors='ignore')\n",
        "\n",
        "# Применение обученной модели (например, RandomForest)\n",
        "y_pred_real_rf = rf_model.predict(X_real)\n",
        "\n",
        "# Применение обученной модели (например, SVM)\n",
        "y_pred_real_svm = svm_model.predict(X_real)\n",
        "\n",
        "# Применение обученной модели (например, XGBoost)\n",
        "y_pred_real_xgb = xgb_model.predict(X_real)\n",
        "\n",
        "# Добавляем предсказания в исходный DataFrame\n",
        "real_data['predicted_is_header_rf'] = y_pred_real_rf\n",
        "real_data['predicted_is_header_svm'] = y_pred_real_svm\n",
        "real_data['predicted_is_header_xgb'] = y_pred_real_xgb\n",
        "\n",
        "# Сохраняем результаты в CSV\n",
        "real_data.to_csv('/content/real_data_with_features.csv', index=False)\n",
        "\n",
        "print(\"Прогнозы сохранены в файл 'predicted_real_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "JjNPX5ywfvbh",
        "outputId": "0345699d-98fc-4d39-d45b-790538891453"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- row_index\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e92f0c217126>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Применение обученной модели (например, RandomForest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_pred_real_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Применение обученной модели (например, SVM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mforce_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \"\"\"\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 )\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     def _validate_data(\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- row_index\n"
          ]
        }
      ]
    }
  ]
}